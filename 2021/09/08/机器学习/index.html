<!DOCTYPE html>
<html lang="zh-CN">

<head>
    <meta charset="UTF-8">
<meta name="viewport"
      content="width=device-width, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">

    <meta name="author" content="NickAdans">





<title>机器学习笔记 | Nick&#39;s Blog</title>



    <link rel="icon" href="/favicon.png">




    <!-- stylesheets list from _config.yml -->
    
    <link rel="stylesheet" href="/css/style.css">
    



    <!-- scripts list from _config.yml -->
    
    <script src="/js/script.js"></script>
    
    <script src="/js/tocbot.min.js"></script>
    



    
    
        
            <!-- MathJax配置，可通过单美元符号书写行内公式等 -->
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    "HTML-CSS": {
        preferredFont: "TeX",
        availableFonts: ["STIX","TeX"],
        linebreaks: { automatic:true },
        EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50)
    },
    tex2jax: {
        inlineMath: [ ["$", "$"], ["\\(","\\)"] ],
        processEscapes: true,
        ignoreClass: "tex2jax_ignore|dno",
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
        equationNumbers: { autoNumber: "AMS" },
        noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } },
        Macros: { href: "{}" }
    },
    messageStyle: "none"
    });
</script>
<!-- 给MathJax元素添加has-jax class -->
<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>
<!-- 通过连接CDN加载MathJax的js代码 -->
<script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
</script>


        
    


<meta name="generator" content="Hexo 5.4.0"></head>

<body>
    <script>
        // this function is used to check current theme before page loaded.
        (() => {
            const currentTheme = window.localStorage && window.localStorage.getItem('theme') || '';
            const isDark = currentTheme === 'dark';
            const pagebody = document.getElementsByTagName('body')[0]
            if (isDark) {
                pagebody.classList.add('dark-theme');
                // mobile
                document.getElementById("mobile-toggle-theme").innerText = "· Dark"
            } else {
                pagebody.classList.remove('dark-theme');
                // mobile
                document.getElementById("mobile-toggle-theme").innerText = "· Light"
            }
        })();
    </script>

    <div class="wrapper">
        <header>
    <nav class="navbar">
        <div class="container">
            <div class="navbar-header header-logo"><a href="/">NICKADANS</a></div>
            <div class="menu navbar-right">
                
                    <a class="menu-item" href="/archives">Posts</a>
                
                    <a class="menu-item" href="/category">Categories</a>
                
                    <a class="menu-item" href="/tag">Tags</a>
                
                    <a class="menu-item" href="/about">About</a>
                
                <input id="switch_default" type="checkbox" class="switch_default">
                <label for="switch_default" class="toggleBtn"></label>
            </div>
        </div>
    </nav>

    
    <nav class="navbar-mobile" id="nav-mobile">
        <div class="container">
            <div class="navbar-header">
                <div>
                    <a href="/">NICKADANS</a><a id="mobile-toggle-theme">·&nbsp;Light</a>
                </div>
                <div class="menu-toggle" onclick="mobileBtn()">&#9776; Menu</div>
            </div>
            <div class="menu" id="mobile-menu">
                
                    <a class="menu-item" href="/archives">Posts</a>
                
                    <a class="menu-item" href="/category">Categories</a>
                
                    <a class="menu-item" href="/tag">Tags</a>
                
                    <a class="menu-item" href="/about">About</a>
                
            </div>
        </div>
    </nav>

</header>
<script>
    var mobileBtn = function f() {
        var toggleMenu = document.getElementsByClassName("menu-toggle")[0];
        var mobileMenu = document.getElementById("mobile-menu");
        if(toggleMenu.classList.contains("active")){
           toggleMenu.classList.remove("active")
            mobileMenu.classList.remove("active")
        }else{
            toggleMenu.classList.add("active")
            mobileMenu.classList.add("active")
        }
    }
</script>
            <div class="main">
                <div class="container">
    
    
        <div class="post-toc">
    <div class="tocbot-list">
    </div>
    <div class="tocbot-list-menu">
        <a class="tocbot-toc-expand" onclick="expand_toc()">Expand all</a>
        <a onclick="go_top()">Back to top</a>
        <a onclick="go_bottom()">Go to bottom</a>
    </div>
</div>

<script>
    document.ready(
        function () {
            tocbot.init({
                tocSelector: '.tocbot-list',
                contentSelector: '.post-content',
                headingSelector: 'h1, h2, h3, h4, h5',
                collapseDepth: 1,
                orderedList: false,
                scrollSmooth: true,
            })
        }
    )

    function expand_toc() {
        var b = document.querySelector(".tocbot-toc-expand");
        tocbot.init({
            tocSelector: '.tocbot-list',
            contentSelector: '.post-content',
            headingSelector: 'h1, h2, h3, h4, h5',
            collapseDepth: 6,
            orderedList: false,
            scrollSmooth: true,
        });
        b.setAttribute("onclick", "collapse_toc()");
        b.innerHTML = "Collapse all"
    }

    function collapse_toc() {
        var b = document.querySelector(".tocbot-toc-expand");
        tocbot.init({
            tocSelector: '.tocbot-list',
            contentSelector: '.post-content',
            headingSelector: 'h1, h2, h3, h4, h5',
            collapseDepth: 1,
            orderedList: false,
            scrollSmooth: true,
        });
        b.setAttribute("onclick", "expand_toc()");
        b.innerHTML = "Expand all"
    }

    function go_top() {
        window.scrollTo(0, 0);
    }

    function go_bottom() {
        window.scrollTo(0, document.body.scrollHeight);
    }

</script>
    

    
    <article class="post-wrap">
        <header class="post-header">
            <h1 class="post-title">机器学习笔记</h1>
            
                <div class="post-meta">
                    
                        Author: <a itemprop="author" rel="author" href="/">NickAdans</a>
                    

                    
                        <span class="post-time">
                        Date: <a href="#">九月 8, 2021&nbsp;&nbsp;10:39:31</a>
                        </span>
                    
                    
                        <span class="post-category">
                    Category:
                            
                                <a href="/categories/%E7%90%86%E8%AE%BA/">理论</a>
                            
                        </span>
                    
                </div>
            
        </header>

        <div class="post-content">
            <h2 id="0906"><a href="#0906" class="headerlink" title="0906"></a>0906</h2><h3 id="监督学习"><a href="#监督学习" class="headerlink" title="监督学习"></a>监督学习</h3><ul>
<li>概念：根据经验E，达成任务T，用指标P来衡量——基于过往的“正确答案”预测新的“正确答案”</li>
<li>应用：预测、分类等</li>
</ul>
<h3 id="无监督学习"><a href="#无监督学习" class="headerlink" title="无监督学习"></a>无监督学习</h3><ul>
<li>概念：使机器从数据集中归纳出数据的特征</li>
<li>应用：聚类、提取不同的音轨等</li>
</ul>
<h3 id="线性回归模型"><a href="#线性回归模型" class="headerlink" title="线性回归模型"></a>线性回归模型</h3><ul>
<li><p>模型：<br>$$<br>h_\theta(x) = \theta_0 + \theta_1x<br>$$</p>
</li>
<li><p>概念：合理选择θ<sub>0</sub>和θ<sub>1</sub>的值，使h<sub>θ</sub>(x)预测的值最接近于训练集中的样本(x,y)<br>$$<br>min\ \frac{1}{2m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2<br>$$<br>其中，m是样本个数，x<sup>(i)</sup>为第i个样本的特征数据，y<sup>(i)</sup>指第i个样本的值</p>
</li>
<li><p>平方误差代价函数：<br>$$<br>J(\theta_0,\theta_1)=\frac{1}{2m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2<br>$$</p>
</li>
<li><p>目标：最小化代价函数 J</p>
</li>
</ul>
<h3 id="梯度下降算法-Gradient-descent"><a href="#梯度下降算法-Gradient-descent" class="headerlink" title="梯度下降算法(Gradient descent)"></a>梯度下降算法(Gradient descent)</h3><ul>
<li><p>算法：给定θ<sub>0</sub>和θ<sub>1</sub>的初始值，不断改变θ<sub>0</sub>和θ<sub>1</sub>来减小代价函数 J ，直到找到 J 的<strong>局部</strong>最小值为止<br>  $$<br>  \begin{split}<br>  &amp;repeat\ until\ convergence:\ \newline<br>  &amp;\quad\theta_j:=\theta_j-\alpha\frac{\partial}{\partial\theta_j}J(\theta) \quad(for\ j=0,1) \newline<br>  \end{split}<br>  $$<br>  其中α被称为学习率，用于控制梯度下降的速度</p>
</li>
<li><p>同步更新(先一起计算，后一起赋值)：<br>$$<br>\begin{split}<br>temp0 &amp;:= \theta_0 - \alpha\frac{\partial}{\partial\theta_0}J(\theta_0,\theta_1) \newline<br>temp1 &amp;:= \theta_1 - \alpha\frac{\partial}{\partial\theta_1}J(\theta_0,\theta_1) \newline<br>\theta_0 &amp;:= temp0 \newline<br>\theta_1 &amp;:= temp1 \newline<br>\end{split}<br>$$</p>
</li>
<li><p><strong>注意</strong>：如果α太小，算法需要经过<strong>特别多步</strong>才能接近最优点，如果α太大，算法会在最优点之间震荡甚至<strong>发散</strong></p>
</li>
<li><p>也称为Batch Gradient descent，意为：每次梯度下降都需要遍历所有样本</p>
</li>
</ul>
<h3 id="线性回归的梯度下降算法"><a href="#线性回归的梯度下降算法" class="headerlink" title="线性回归的梯度下降算法"></a>线性回归的梯度下降算法</h3><ul>
<li>算法：</li>
</ul>
<p>$$<br>\begin{split}<br>\frac{\partial}{\partial\theta_0}J(\theta_0,\theta_1) &amp;= \frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)}) \newline<br>\frac{\partial}{\partial\theta_1}J(\theta_0,\theta_1) &amp;= \frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})·x^{(i)} \newline<br>\theta_0:&amp;=\theta_0-\alpha\frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)}) \newline<br>\theta_1:&amp;=\theta_1-\alpha\frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})·x^{(i)}<br>\end{split}<br>$$</p>
<ul>
<li>由于线性回归的代价函数是<strong>凸函数</strong>，因此梯度下降的<strong>局部最优点</strong>即<strong>全局最优点</strong></li>
</ul>
<h2 id="0908"><a href="#0908" class="headerlink" title="0908"></a>0908</h2><h3 id="多元线性回归-多特征的线性回归"><a href="#多元线性回归-多特征的线性回归" class="headerlink" title="多元线性回归(多特征的线性回归)"></a>多元线性回归(多特征的线性回归)</h3><ul>
<li>模型：<br>$$<br>\begin{split}<br>&amp;x =\begin{bmatrix}x_0\newline x_1\newline x_2\newline \vdots\newline x_n\end{bmatrix}\in R^{n+1}, \ x_0=1 \ \ and \ <br>\theta=\begin{bmatrix}\theta_0 \newline \theta_1 \newline \theta_2\newline \vdots\newline \theta_n\end{bmatrix}\in R^{n+1} \newline\newline<br>&amp;h_\theta(x) = \theta_0+\theta_1x_1+\theta_2x_2+…+\theta_nx_n\newline<br>\newline<br>&amp;h_\theta(x) = \theta^Tx<br>\end{split}<br>$$<br>注意，n为特征个数，但由于x<sub>0</sub>=1，实际上向量x和θ属于n+1维向量</li>
<li>代价函数：<br>$$<br>J(\theta)=\frac{1}{2m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2<br>$$</li>
<li>梯度下降：<br>$$<br>\begin{split}<br>&amp;repeat\ until\ convergence:\ \newline<br>&amp;\quad\theta_j:=\theta_j-\alpha\frac{\partial}{\partial\theta_j}J(\theta) \quad(for\ j=0,1,…,n) \newline<br>&amp;means:\newline<br>&amp;\quad\theta_j:=\theta_j-\alpha\frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}<br>\end{split}<br>$$</li>
</ul>
<h3 id="特征缩放-Feature-Scaling"><a href="#特征缩放-Feature-Scaling" class="headerlink" title="特征缩放(Feature Scaling)"></a>特征缩放(Feature Scaling)</h3><ul>
<li><p>思路：Get every feature into approximately a -1&lt;=x<sub>i</sub>&lt;+1 range.即：对每个特征，采用归一化处理，使不同特征的取值处于近似的范围，从而让梯度下降的速度变快。</p>
</li>
<li><p>算法：<br>$$<br>\begin{split}<br>x_i := \frac{x_i-\mu_i}{s_i} \newline<br>\end{split}<br>$$<br>其中，μ<sub>i</sub>是训练集中x<sub>i</sub>的均值，而s<sub>i</sub>是x<sub>i</sub>的取值范围(即x<sub>i</sub>的最大值减去x<sub>i</sub>的最小值)</p>
</li>
</ul>
<h3 id="选取合适的α-学习率"><a href="#选取合适的α-学习率" class="headerlink" title="选取合适的α(学习率)"></a>选取合适的α(学习率)</h3><ul>
<li>确定梯度下降算法是否收敛的方式：<ul>
<li>画minJ(θ)—No. of itera曲线图，并通过曲线图判断</li>
<li>自动收敛测试：通过声明一个最小值ε，当每次梯度下降的变化值小于ε时，认为梯度下降已收敛</li>
</ul>
</li>
<li>当学习率过小，每次梯度下降变化的幅度较小，收敛十分缓慢</li>
<li>当学习率过大，每次梯度下降变化的幅度较大，可能会震荡乃至发散</li>
<li>在线性回归中，通常从较小的学习率开始，不断乘三，确定学习率的最大值后，再略微下调，即可找到满意的学习率<ul>
<li>如：0.0001 -&gt; 0.0003 -&gt; 0.001 -&gt; 0.003 -&gt; 0.01 -&gt; 0.03 -&gt; 0.1 -&gt; 0.3 -&gt; 0.28 -&gt; 0.25</li>
</ul>
</li>
</ul>
<h3 id="多项式回归"><a href="#多项式回归" class="headerlink" title="多项式回归"></a>多项式回归</h3><ul>
<li>思路：通过自由选择特征，使用更复杂的函数拟合数据</li>
</ul>
<h3 id="正规方程法解决线性回归问题"><a href="#正规方程法解决线性回归问题" class="headerlink" title="正规方程法解决线性回归问题"></a>正规方程法解决线性回归问题</h3><ul>
<li><p>思路：<br>$$<br>\frac{\partial}{\partial\theta_j}J(\theta)=…=0\quad(for\ j=0,1,…,n)<br>$$</p>
</li>
<li><p>算法：<br>$$<br>\begin{split}<br>x&amp;=\begin{bmatrix}x_0\newline x_1\newline x_2\newline \vdots\newline x_n\end{bmatrix}\in R^{n+1}, \ x_0=1 \ \ and \ <br>\theta=\begin{bmatrix}\theta_0 \newline \theta_1 \newline \theta_2\newline \vdots\newline \theta_n\end{bmatrix}\in R^{n+1} \newline\newline<br>X&amp;=\begin{bmatrix}(x^{(1)})^T\newline( x^{(2)})^T\newline \vdots \newline (x^{(n)})^T\newline \end{bmatrix}, \<br>y=\begin{bmatrix}y^{(1)}\newline y^{(2)}\newline \vdots\newline y^{(n)}\end{bmatrix}\newline\newline<br>\theta&amp;=(X^TX)^{-1}X^Ty<br>\end{split}<br>$$<br>其中，X为样本的特征矩阵，y为样本的值向量</p>
</li>
<li><p>使用该方法时，无需进行特征缩放</p>
</li>
<li><p>当特征个数n&lt;10000时，通常使用正规方程法；否则，使用梯度下降法</p>
</li>
<li><p>当X<sup>T</sup>X不可逆的时候，检查是否有冗余特征，或者删除一些特征数，或使用正规化方法</p>
</li>
</ul>
<h2 id="0911"><a href="#0911" class="headerlink" title="0911"></a>0911</h2><h3 id="逻辑回归-分类"><a href="#逻辑回归-分类" class="headerlink" title="逻辑回归(分类)"></a>逻辑回归(分类)</h3><ul>
<li><p>模型：<br>$$<br>\begin{split}<br>&amp;g(z)=\frac{1}{1+e^{-z}}\quad(if\ z\in R)\newline \newline<br>&amp;h_\theta(x)=g(\theta^Tx)=\frac{1}{1+e^{-\theta^Tx}}\newline \newline<br>&amp;Want\quad 0\leq h_\theta(x)\leq 1<br>\end{split}<br>$$</p>
<p>其中g(z)也叫sigmoid function(logistic function)</p>
<p><img src="https://z3.ax1x.com/2021/09/11/hzEqX9.png" alt="hzEqX9.png"></p>
<center>图1.sigmoid function</center></li>
<li><p>假设：<br>$$<br>\begin{split}<br>\because the\ real\ value\ &amp;of\ y\ is\ always\ 0\ or\ 1,\ so:\newline<br>predict\ y&amp;=1\ if\ h_\theta(x)\ge0.5\newline<br>predict\ y&amp;=0\ if\ h_\theta(x)&lt;0.5\newline<br>\because h_\theta(x)&amp;=g(\theta^Tx)\newline<br>it\ means:\ when\ \theta^Tx&amp;&lt;0\ h_\theta(x) &lt; 0.5,\ y=0\newline<br>\end{split}<br>$$</p>
</li>
<li><p>决策边界：<br>$$<br>\begin{split}<br>&amp;the\ line\ which\ separates\ the\ region\ of\ y:\newline<br>&amp;\theta_0+\theta_1x_1+\theta_2x_2+…\theta_nx_n\geq 0\newline<br>\end{split}<br>$$</p>
</li>
<li><p>不适合使用的代价函数：<br>$$<br>\begin{split}<br>Cost(h_\theta(x),\ y)&amp;=\frac{1}{2}(h_\theta(x)-y)^2\newline<br>J(\theta)&amp;=\frac{1}{m}\sum_{i=1}^m\frac{1}{2}(h_\theta(x^{(i)})-y^{(i)})^2\newline<br>&amp;=\frac{1}{m}\sum_{i=1}^mCost(h_\theta(x^{(i)}),\ y^{(i)})\newline<br>\end{split}<br>$$<br>该代价函数在逻辑回归的h<sub>θ</sub>(x)下表现为非凸函数，需要另寻凸函数，才方便使用梯度下降算法。</p>
</li>
</ul>
<h3 id="逻辑回归使用的代价函数："><a href="#逻辑回归使用的代价函数：" class="headerlink" title="逻辑回归使用的代价函数："></a>逻辑回归使用的代价函数：</h3><ul>
<li><p>模型：<br>$$<br>Cost(h_\theta(x),\ y)={<br>\begin{split}<br>&amp;-log(h_\theta(x))\quad if\ y=1\newline<br>&amp;-log(1-h_\theta(x))\quad if\ y=0\newline<br>\end{split}<br>$$</p>
</li>
<li><p>简化代价函数：<br>$$<br>\begin{split}<br>Cost(h_\theta(x),\ y)&amp;=-ylog(h_\theta(x))-(1-y)log(1-h_\theta(x))\newline<br>J(\theta)&amp;=-\frac{1}{m}\sum_{i=1}^m-Cost(h_\theta(x^{(i)}),\ y^{(i)})\newline<br>J(\theta)&amp;=-\frac{1}{m}\sum_{i=1}^my^{(i)}log(h_\theta(x^{(i)}))+(1-y^{(i)})log(1-h_\theta(x^{(i)}))\newline<br>\end{split}<br>$$</p>
</li>
<li><p>梯度下降：<br>$$<br>\begin{split}<br>&amp;repeat\ until\ convergence:\ \newline<br>&amp;\quad\theta_j:=\theta_j-\alpha\frac{\partial}{\partial\theta_j}J(\theta) \quad(for\ j=0,1,…,n) \newline<br>&amp;means:\newline<br>&amp;\quad\theta_j:=\theta_j-\alpha\frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}<br>\end{split}<br>$$</p>
</li>
</ul>
<h3 id="高级优化方法"><a href="#高级优化方法" class="headerlink" title="高级优化方法"></a>高级优化方法</h3><ul>
<li>gradient descent</li>
<li>conjugate gradient</li>
<li>BFGS</li>
<li>L-BFGS</li>
</ul>
<h2 id="0912"><a href="#0912" class="headerlink" title="0912"></a>0912</h2><h3 id="多元分类"><a href="#多元分类" class="headerlink" title="多元分类"></a>多元分类</h3><ul>
<li>思路：将多元分类转化成多个二元分类的问题。为每个分类 i 训练一个逻辑回归的分类器 h<sub>θ</sub><sup>(i)</sup>(x)，从而预测 y=i 的概率。即，每当一个新的 x 值输入时，遍历每个 h<sub>θ</sub><sup>(i)</sup>(x)，选出最大的概率值，即预测了 y 可信度最高的值。</li>
</ul>
<h3 id="过拟合-overfitting-问题"><a href="#过拟合-overfitting-问题" class="headerlink" title="过拟合(overfitting)问题"></a>过拟合(overfitting)问题</h3><ul>
<li>解决办法：减少特征的数量（使用人工删减的办法，或者模型选择算法）</li>
<li>正则化：保留所有的特征，减少参数θ的量级。</li>
</ul>
<h3 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h3><ul>
<li><p>选择一个合适的正则化参数λ来修正损失函数：<br>$$<br>J(\theta)=\frac{1}{2m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2+\frac{\lambda}{2m}\sum_{j=1}^n\theta_j^2<br>$$</p>
<p>注意，这里的 j 是从<font color=red> <strong>1</strong> </font>开始的，意味着 θ<sub>0</sub> 不作正则处理</p>
</li>
<li><p>正则化的线性回归：</p>
<ul>
<li>梯度下降：<br>$$<br>\begin{split}<br>&amp;repeat\ until\ convergence:\ \newline<br>&amp;\quad\theta_j:=\theta_j-\alpha\frac{\partial}{\partial\theta_j}J(\theta) \quad(for\ j=0,1,…,n) \newline<br>&amp;means:\newline<br>&amp;\quad\theta_0:=\theta_0-\alpha\frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_0^{(i)}\newline<br>&amp;\quad\theta_j:=\theta_j-\alpha\frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}+\frac{\lambda}{m}\theta_j\quad(j=1,…,n)\newline<br>&amp;\quad\theta_j:=(1-\alpha\frac{\lambda}{m})\theta_j-\alpha\frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}\quad(j=1,…,n)\newline<br>\end{split}<br>$$</li>
</ul>
</li>
<li><p>正规方程法：<br>  $$<br>  \begin{split}<br>  x&amp;=\begin{bmatrix}x_0\newline x_1\newline x_2\newline \vdots\newline x_n\end{bmatrix}\in R^{n+1}, \ x_0=1 \ \ and \ <br>  \theta=\begin{bmatrix}\theta_0 \newline \theta_1 \newline \theta_2\newline \vdots\newline \theta_n\end{bmatrix}\in R^{n+1} \newline\newline<br>  X&amp;=\begin{bmatrix}(x^{(1)})^T\newline( x^{(2)})^T\newline \vdots \newline (x^{(n)})^T\newline \end{bmatrix}, \<br>  y=\begin{bmatrix}y^{(1)}\newline y^{(2)}\newline \vdots\newline y^{(n)}\end{bmatrix}\newline\newline<br>  \theta&amp;=(X^TX+\lambda<br>  \begin{bmatrix}<br>  0 &amp; 0 &amp; \cdots &amp;  0\newline<br>  0 &amp; 1 &amp; \cdots &amp;  0\newline<br>  \vdots &amp; \vdots &amp; \ddots &amp; \newline<br>  0 &amp; 0 &amp; &amp; 1<br>  \end{bmatrix}<br>  )^{-1}X^Ty\quad(if\ \lambda &gt; 0\ and\ m\leq n)<br>  \end{split}<br>  $$</p>
<p>注意，使用正规方程法，需要正则化系数 λ &gt; 0 且样本个数 m 小等于特征个数 n </p>
</li>
<li><p>正则化的逻辑回归：</p>
<ul>
<li>梯度下降(与线性回归相似)：<br>$$<br>\begin{split}<br>&amp;repeat\ until\ convergence:\ \newline<br>&amp;\quad\theta_j:=\theta_j-\alpha\frac{\partial}{\partial\theta_j}J(\theta) \quad(for\ j=0,1,…,n) \newline<br>&amp;means:\newline<br>&amp;\quad\theta_0:=\theta_0-\alpha\frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_0^{(i)}\newline<br>&amp;\quad\theta_j:=\theta_j-\alpha\frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}+\frac{\lambda}{m}\theta_j\quad(j=1,…,n)\newline<br>&amp;\quad\theta_j:=(1-\alpha\frac{\lambda}{m})\theta_j-\alpha\frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}\quad(j=1,…,n)\newline<br>\end{split}<br>$$</li>
</ul>
</li>
</ul>
<h3 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h3><ul>
<li><p>模型：逻辑神经单元</p>
<p><img src="https://z3.ax1x.com/2021/09/12/4pRy8J.jpg" alt="4pRy8J.jpg"></p>
<center>图2.逻辑神经元</center></li>
<li><p>激活函数：<br>$$<br>g(z)=\frac{1}{1+e^{-z}}<br>$$</p>
</li>
<li><p>神经网络示意图：</p>
<p><img src="https://z3.ax1x.com/2021/09/12/4pWmiF.jpg" alt="4pWmiF.jpg"></p>
<center>图3.逻辑神经网络</center>

<ul>
<li>x<sub>0</sub>、α<sub>0</sub><sup>(2)</sup>为偏置单元(bias unit)，默认值为1</li>
<li>第一层为输入层，最后一层为输出层，非输入层和输出层的中间层称为隐藏层(hidden layer)</li>
</ul>
<p>$$<br>\begin{split}</p>
<pre><code>a_i^&#123;(j)&#125;&amp;=activation\ of\ unit\ \ &#39;i&#39;\ \ in\ layer\ \ &#39;j&#39;\newline
\Theta^&#123;(j)&#125;&amp;=matrix\ of\ weights\ controlling\ function\ mapping\ from\ layer\ \ &#39;j&#39;\ \ to\ layer\ \ &#39;j+1&#39;
</code></pre>
<p>\end{split}<br>$$</p>
<p>​    Θ<sup>(j)</sup> 也称为<strong>权重矩阵</strong></p>
<ul>
<li><p>图3表示的计算：<br>$$<br>\begin{split}<br>\alpha_1^{(2)}&amp;=g(\Theta_{10}^{(1)}x_0+\Theta_{11}^{(1)}x_1+\Theta_{12}^{(1)}x_2+\Theta_{13}^{(1)}x_3)\newline<br>\alpha_2^{(2)}&amp;=g(\Theta_{20}^{(1)}x_0+\Theta_{21}^{(1)}x_1+\Theta_{22}^{(1)}x_2+\Theta_{23}^{(1)}x_3)\newline<br>\alpha_3^{(2)}&amp;=g(\Theta_{30}^{(1)}x_0+\Theta_{31}^{(1)}x_1+\Theta_{32}^{(1)}x_2+\Theta_{33}^{(1)}x_3)\newline<br>h_\Theta(x)&amp;=\alpha_1^{(3)}=g(\Theta_{10}^{(2)}a_0^{(2)}+\Theta_{11}^{(1)}a_1^{(2)}+\Theta_{12}^{(1)}a_2^{(2)}+\Theta_{13}^{(1)}a_3^{(2)})\newline<br>\end{split}<br>$$</p>
</li>
<li><p>一般化：如果神经网络在 j 层有 s<sub>j</sub> 个单元，在 j+1 层有 s<sub>j+1</sub> 个单元，则 Θ<sup>(j)</sup> 的纬度是 s<sub>j+1</sub> x (s<sub>j</sub> + 1)<br>$$<br>\begin{split}<br>Generally:\ if\ network\ has\ s_j\ units\ in\ layer\ j,\ s_{j+1}\ units\ in\ layer\ j+1,\ then\ \Theta^{(j)}\ will\ be\ of\ dimension\ s_{j+1}\times(s_j+1)<br>\end{split}<br>$$</p>
</li>
</ul>
</li>
</ul>

        </div>

        
            <section class="post-copyright">
                
                    <p class="copyright-item">
                        <span>Author:</span>
                        <span>NickAdans</span>
                    </p>
                
                
                    <p class="copyright-item">
                        <span>Permalink:</span>
                        <span><a href="https://nickadans.com/2021/09/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">https://nickadans.com/2021/09/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/</a></span>
                    </p>
                
                
                    <p class="copyright-item">
                        <span>License:</span>
                        <span>Copyright (c) 2019 <a target="_blank" rel="noopener" href="http://creativecommons.org/licenses/by-nc/4.0/">CC-BY-NC-4.0</a> LICENSE</span>
                    </p>
                
                

            </section>
        
        <section class="post-tags">
            <div>
                <span>Tag(s):</span>
                <span class="tag">
                    
                    
                        <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"># 机器学习</a>
                    
                        <a href="/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"># 人工智能</a>
                    
                        
                </span>
            </div>
            <div>
                <a href="javascript:window.history.back();">back</a>
                <span>· </span>
                <a href="/">home</a>
            </div>
        </section>
        <section class="post-nav">
            
            
            <a class="next" rel="next" href="/2021/08/08/%E5%86%99%E5%9C%A8%E4%BA%8C%E5%8D%81%E4%B8%89%E5%B2%81%E5%88%9D/">写在二十三岁初</a>
            
        </section>


    </article>
</div>

            </div>
            <footer id="footer" class="footer">
    <div class="copyright">
        <span>© NickAdans | Powered by <a href="https://hexo.io" target="_blank">Hexo</a> & <a href="https://github.com/Siricee/hexo-theme-Chic" target="_blank">Chic</a></span>
    </div>
</footer>

    </div>
</body>

</html>